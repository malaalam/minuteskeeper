---
layout: post
title: Pose Estimation
categories: Computer Vision
tags:
mathjax: true
comments: true
---
 
(From [Pischulin et al, 2019](https://arxiv.org/abs/1511.06645))
> "Key challenges inherent to multi person pose estimation are the partial visibility of some people, significant overlap of bounding box regions of people, and the a-priori unknown number of people in an image.  The problem thus is to infer the number of persons, assign part detections to person instances while respecting geometric and appearance constraints. Most strategies use a two-stage inference process to first detect and then independently estimate poses. This is unsuited for cases when people are in close proximity since they permit simulataneous assignment of the same body-part candidates to multiple people hypotheses."

<p float="center"><figure>
<img src="../images/2019-08-10/imageOne.jpg" width= "300" />
<img src="../images/2019-08-10/imageTwo.png" width ="300"/>
<figcaption>
[Figure One : From left to right (a) An image containing multiple people (b) Joints are predicted by Deeper Cut]</figcaption></figure>
</p>

Recently, I attempted to employ [Deeper Cut](https://arxiv.org/abs/1605.03170), which has traditionally been used for the purpose of human pose estimation, to instead, locate the *joints* of a *Drosophila* specimen. The broader goal of this task is to localize the cephalic furrow and to determine the proportion in which it divides the anterior-posterior axis. I proposed that considering the ends of the cephalic furrow as *joints* could serve as a proxy-task for obtaining the location of the furrow.

## Data Preparation

I downloaded 6 volumetric images, corresponding to the time points from 100 to 150, with a step of 10 and downscaled the images by a factor of 4 as the first step. Next, using the Single Point annotation tool in Fiji, I marked four joints corresponding to the Head, Furrow-End 1, Tail and Furrow-End 2 in all slices where these joints appeared easy to spot (around 40 slices per volumetric image). It helps to set the contrast to the maximum possible for clearer delineation of boundaries, while performing the annotation.

I next wrote some scripts to convert the annotations to a *dataset.mat*, which is used as input by the config file for training. The config file attributes are currently kept the same as default: would try to investigate these more closely in the later reruns. The first five volumetic images were converted to sliced, 2-D images and used for training, while the last volumetric image (time point 150) was used for testing the predictions.







