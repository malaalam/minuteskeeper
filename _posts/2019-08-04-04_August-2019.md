---
layout: post
title: Training on the Fly 
categories: Computer Vision
tags:
mathjax: true
comments: true
---
 
(From [Krull et al, 2019](https://arxiv.org/pdf/1906.00651.pdf))
> "N2V proposes a way to leverage information of the networkâ€™s blind-spots. However, PN2V is not restricted to Gaussian noise models or Gaussian intensity predictions. More precisely, to compute the posterior distribution of a pixel, we combine (i) a general noise model that can be represented as a histogram (observation likelihood), and (ii) a distribution of possible truepixel intensities (prior), represented by a set of predicted samples."

Recently, we attempted to investigate whether the noise model of Probabilistic Noise2Void which was defined as a 256 times 256 sized-histogram could be replaced by a fewer parameter definition. One intuition behind why this may be a good idea is because firstly, the variance of the observations given the signal seems to follow a linear function of the signal:
i.e. $$ \sigma^{2}_{x|s} = e^{a} + e^{b} \times s $$ 
and secondly, the observations given the signal seem to follow a unimodal distribution. (We postulate that such a unimodal distribution can be captured through representation as a unimodal gaussian: 
i.e. $$ P(x|s) \simeq N(s, \sigma_{x|s}))$$

In the early days while implementing this, we initialized $$a$$ to be equal to 6 and $$b$$ to  a random value between -1 and 1, and also normalized $$s$$ by a factor 10000. This implies that the new equation showing linear dependence would be:
$$ \sigma^{2}_{x|s} = e^{a} + e^{b} \times \hat{s} $$, 
where $$\hat{s} =\frac{s}{10000}$$.
Such a formulation worked well in a supervised setup (where you have both the observations and the signal). However, it produced very narrow noise models $$p(x|\hat{s})$$ during on-the-fly training.

I reasoned that if one were to assume perfect Poisson noise, then $$b$$ would equal $$log(10000)$$. Also, we thought it would be a good idea to have a separate learning rate for the denoising parameters and the noise model parameters. This is easily implemented as follows:

```python
optimizer=optim.Adam([
                {'params': net.parameters()},
                {'params': noiseModel.weight, 'lr': 1e-2}
            ], lr=learningRate)
```


 


